Work diary for Zhiyue Pan z5212889

Week 1
Group formed. I created the Jira & GitHub accounts. 

Together with my group mates, I wrote the user story and sprints section of the proposal. I also found and discussed with the team all available software tools and libraries that we can use for the project. We decided that we will do an movie review system, we discussed about the functions that out system will provide, and the advantages our system will have over the existing website —rotten tomato. 

Week2 
This week I was working on the proposal. I completed my part which includes user story, description and How the objectives being satisfised for three main functions. I also completed the story board durig lab with my teammates, and complete the rest of the parts at home. 

week 3 
During lab: 
Finalise the story board with my groupmates, and moving the user stories we worte on google doc into jira.

5/03/2023
Complete the Sprints section and the novel functionalities section for the proposal. The novel functionality section includes 9 sections of Reward point system:,Trailer on the front page:,preference system:,banned review writer(s):,More precise recommender system:,Goal setting:,Dark/light mode and Advanced Review metrics. Also finalise the proposal alonge with Yuemeng Yin. 

Week3
This week I starting working on the coding part that I was assigned to. My task for this program was to generate data that will be later used for our movie review website. The datas that needs to be generate includes three parts. The first part is the movie information that includes 'movie id', 'name', 'director', 'actor', 'release date', 'cover link', 'details', 'type' and 'tag'. The second part of informations that needs to be generate is reviews. As we are a movie review website, each movie needs to have large amount of comments so that users could use these informations to choose the movies that they might watch later. The informations that related to the movie generation includes 'Comment ID', 'User ID', 'Movie ID', 'Star Rating (0-5)', 'Comment Text' ‘like' 'dislike'. The third parts of informations that needs to be generated refers to the users. The users need to have user name and user id. 
The group leader give me two choices to get this job done. The first one is to manually copy all the informations down from the existing movie review website, and the other one was to write a A web crawler capable of automatically scraping data from movie websites. The first method seems much easier but requires larger amount of time as we need a minimal of 30 movies and each movie needs 100-300 comments. The second method obviously takes less time and would allow our team to generate as much informations as we want. However, as there wasn't a course that taught us to write a crawler, this will be a very challenging task. 
However, consider the nature of this project is computer science, and having a crawler would make our website much better. I decided to go with the second option. 


week3 Sat+Sun
Today I tried to write the crawler code.

week4 Wed
Today I completed the first version of my crawler. This crawer is written in python and is able to generate the movie informations after putting the movie link. 

week4 Sat+SUn
Today I was required by the frontend group to have a new function which could download all the movie poster and actor's pictures. So that they could later post the pictures on the movie information page. I worked on the weekends and completed this function. 

Week5 Sat
This week I managed the implement the first version of the image downloading function. I crawed poster, cast pictures with each actor's name for all 70 movies.
I updated the databass and our website can now display all these pictures with correct name.

Week6 
This week I continued to work on my crawler to make it faster. I also continue to generate data and update the database for the rest of the movies.

Week 7 
This week I workinf on extending the database by increase the number of movies to 200. I also modify the crawler to make it generate more comments and exclude the repetated comments. The frontend team asked me to add more zero mark movies to the database. This will make our movie ranking looks better. 

week8 
This week I encourter a challenge. I was planning to complete generating data on Tuesday. But at Tuesday evening, when I only have 20 movies left for the movie data generation, my crawler suddenly runs very slowly and eventually stopped responding. I was very confused as this situation never happened before. I went to the website rotten tomato and found that the website was responding extremly slowly as well. I tried to run and fix the code for 30 minutes but still could not get it generate datas as before. I was very worried as we must finalising everything tonight in order to get the demo presentation go well tomorrow. As the rotten tomato stopped responding, I decidede to manually download the data. It was a very painful experience as I have to copy all the comments, and click to download the pictures for all the actors one by one. But eventually, I managed to generate all the data in time, and our database was successfually updated before the demo. 

Week 8 Sat
Right after the wed demo, I had another looked at my code and rotten tomato, and realised there was a big change of the website. The structure and tags has been changed a lot, so my oriangal code could no longer generate movie informations. I rewrite the codes during the weekend and kept working on updating the databse. 

complete retro B

week 9
This week I generated 200+ movies for our database, our database now having more than 300 movies. I particular choosed some famous movies and lastest movies to make our website more realistic and attractive. I then did some data cleaining and put it into the database. 

week 10 Tue
we had a meeting to have a final review for our website, and had a demo presentaion. I wrote 500 words script for the presentation tmr. 

week 10 Tue
Complete 60 pages of final reports. 

Week 10 Fri
Meeting with groupmates and continue finalising the report.
